{"cells":[{"cell_type":"markdown","metadata":{"id":"JLojLUpcGNbk"},"source":["# **차량 공유업체의 차량 파손 여부 분류하기**"]},{"cell_type":"markdown","source":["## 0.미션\n","\n","* 1) 미션1 : Data Preprocessing\n","    - **과제 수행 목표**\n","        - 본인의 구글 드라이브에 모델링 수행을 위해 적절한 폴더 및 파일로 **일관성 있게 정리**해야 합니다.\n","        - 제공된 데이터 : Car_Images.zip\n","            * Car_Images : 차량의 정상/파손 이미지 무작위 수집"],"metadata":{"id":"BbrllJY8JdkF"}},{"cell_type":"markdown","source":["* 2) 미션2 : CNN 모델링\n","    - **과제 수행 목표**\n","        - Tensorflow Keras를 이용하여 모델을 3개 이상 생성하세요.\n","            - 모델 구조와 파라미터는 자유롭게 구성하세요.\n","            - 단, 세부 목차에서 명시한 부분은 지켜주세요."],"metadata":{"id":"Hgdg96jE-mmd"}},{"cell_type":"markdown","source":["* 3) 미션3 : Data Argumentation & Transfer Learning\n","    - **과제 수행 목표**\n","        - 성능 개선을 위해 다음의 두가지를 시도하세요.\n","            * Data Augmentation을 적용하세요.(Image Generator)\n","            * Transfer Learning(VGG16)\n"],"metadata":{"id":"VRrUY4ud_rJV"}},{"cell_type":"markdown","metadata":{"id":"7MdjZtxfGNKz"},"source":["## 1.환경설정 "]},{"cell_type":"markdown","metadata":{"id":"6QgFWzN9xhlr"},"source":["### (1) 데이터셋 폴더 생성\n","- **세부요구사항**\n","    - C드라이브에 Datasets라는 폴더를 만드세요.\n","        - 구글드라이브를 사용하는경우 드라이브 첫 화면에 Datasets 라는 폴더를 만드세요. ('/content/drive/MyDrive/Datasets/')\n","    - 해당 폴더 안에 Car_Images.zip 파일을 넣으세요."]},{"cell_type":"markdown","source":["* 구글 Colab을 이용하는 경우"],"metadata":{"id":"Elg8NL8vwUs5"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"kWUbDvBzwiTq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376132293,"user_tz":-540,"elapsed":28463,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"cfacfd8d-e77f-4a6c-819b-ca7be24aa9d7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!mkdir /content/drive/MyDrive/Datasets"],"metadata":{"id":"D3qJC5lpk8bC","executionInfo":{"status":"ok","timestamp":1679376132293,"user_tz":-540,"elapsed":7,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"1f40ee85-25fc-4ff6-e232-9155b8e03f03","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/drive/MyDrive/Datasets’: File exists\n"]}]},{"cell_type":"markdown","metadata":{"id":"0sVNbCKnLUGc"},"source":["### (2) 데이터셋 불러오기 \n","- **세부요구사항**\n","    - Car_Images.zip 파일을 C:/Datasets/ 경로에 압축 해제합니다.\n","    - zipfile 모듈을 이용하거나 다른 방식을 사용해도 됩니다.\n","        - 참고 자료 : [zipfile document](https://docs.python.org/3/library/zipfile.html#zipfile-objects)\n","    - 폴더구조(로컬)\n","        * C:/Datasets/ : 압축파일\n","        * C:/Datasets/Car_Images_train/ : 압축 해제한 이미지 저장소\n","    - 폴더구조(구글드라이브브)\n","        * /content/drive/MyDrive/Datasets/ : 압축파일\n","        * /content/drive/MyDrive/Datasets/Car_Images_train/ : 압축 해제한 이미지 저장소\n","    - 압축을 해제하면 다음과 같은 두 하위 폴더가 생성됩니다.\n","        * normal, abnormal : 각 폴더에는 이미지들이 있습니다.\n","        * 이후 단계에서 해당 경로로 부터 validation, test 셋을 추출하게 됩니다.\n","        "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"K2-8EaA9x4Xm","executionInfo":{"status":"ok","timestamp":1679376132294,"user_tz":-540,"elapsed":5,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["import zipfile"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hMkstFLKx4Xm","executionInfo":{"status":"ok","timestamp":1679376132294,"user_tz":-540,"elapsed":4,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["# 압축파일 경로\n","# 구글 드라이브인 경우 경로에 맞게 지정하세요.\n","# dataset_path  = '/content/drive/MyDrive/Datasets/'\n","dataset_path = '/content/drive/MyDrive/Datasets/'\n","\n","file_path = dataset_path + 'Car_Images.zip'"]},{"cell_type":"code","source":["!mkdir /content/drive/MyDrive/Datasets/Car_Image_train; "],"metadata":{"id":"7oMqiXatp8ys","executionInfo":{"status":"ok","timestamp":1679376133556,"user_tz":-540,"elapsed":1266,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"sgT_RB14Lwza","executionInfo":{"status":"ok","timestamp":1679376150921,"user_tz":-540,"elapsed":17368,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["# 압축 해제\n","data = zipfile.ZipFile('/content/drive/MyDrive/Car_Images.zip')\n","data.extractall('/content/drive/MyDrive/Datasets/Car_Image_train')"]},{"cell_type":"markdown","metadata":{"id":"8hgC0axQyMhI"},"source":["### (3) 이미지 저장을 위한 폴더 생성\n","- **세부요구사항**\n","    - train, validation, test 을 위해 각각 하위 폴더 normal과 abnormal를 준비합니다.\n","        - train\n","            * 정상 이미지 저장소 : C:/Datasets/Car_Images_train/normal/ \n","                * 구글드라이브 :   /content/drive/MyDrive/Datasets/Car_Images_train/normal/\n","            * 파손 이미지 저장소 : C:/Datasets/Car_Images_train/abnormal/\n","                * 구글드라이브 : /content/drive/MyDrive/Datasets/Car_Images_train/abnormal/\n","        - val, test 역시 동일한 구조로 생성합니다.\n","    - 직접 탐색기에서 폴더를 생성할 수도 있고, os 모듈을 이용하여 코드로 작성할 수도 있습니다.\n","        - 참고 자료 : [os document](https://docs.python.org/3/library/os.html)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Rc8GnuauOzLf","executionInfo":{"status":"ok","timestamp":1679376151702,"user_tz":-540,"elapsed":785,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["# train 폴더는 압축을 해제하면서 이미 생성 되어 있습니다.\n","\n","# test 폴더 만들기 os.mkdir()\n","!mkdir /content/drive/MyDrive/Datasets/Car_Image_test; \n","!mkdir /content/drive/MyDrive/Datasets/Car_Image_test/normal\n","!mkdir /content/drive/MyDrive/Datasets/Car_Image_test/abnormal\n","\n","# validation 폴더 만들기\n","!mkdir /content/drive/MyDrive/Datasets/Car_Image_val; \n","!mkdir /content/drive/MyDrive/Datasets/Car_Image_val/normal\n","!mkdir /content/drive/MyDrive/Datasets/Car_Image_val/abnormal\n"]},{"cell_type":"markdown","metadata":{"id":"FYZKJrP0GtPh"},"source":["## 2.데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"j-ilpDQfInAE"},"source":["### (1) 데이터 분할 : Training set | Validation set | Test set 생성\n","- **세부요구사항**\n","    - Training set, Validation set, Test set을 만듭니다.\n","        * size\n","            * test : 전체에서 20%를 추출합니다.\n","            * validation : test를 떼어낸 나머지에서 다시 20%를 추출합니다.\n","        * 데이터는 랜덤하게 추출해야 합니다.\n","            - random, shutil 모듈을 이용하여 랜덤하게 추출할 수 있습니다.\n","                - [random document](https://docs.python.org/3/library/random.html) | [shutil document](https://docs.python.org/3/library/shutil.html)\n","            * 해당 모듈 이외에 자신이 잘 알고 있는 방법을 사용해도 됩니다.\n","---"]},{"cell_type":"markdown","source":["#### 1) test, validation 크기를 지정"],"metadata":{"id":"mFMSDA26RS-E"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"JhQ_Gu_KNR2g","executionInfo":{"status":"ok","timestamp":1679376151702,"user_tz":-540,"elapsed":11,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["import random, shutil, os\n","import numpy as np"]},{"cell_type":"code","source":["tr_n_path = '/content/drive/MyDrive/Datasets/Car_Image_train/normal/'\n","tr_ab_path = '/content/drive/MyDrive/Datasets/Car_Image_train/abnormal/'"],"metadata":{"id":"1rsajRs12URF","executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":11,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"PdU7X9e70dBu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":11,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"0954b6e7-3e83-4f68-dc02-50dc1a0594b9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(302, 303)"]},"metadata":{},"execution_count":10}],"source":["# 전체 이미지 갯수를 확인합니다.\n","len(os.listdir(tr_n_path)) , len(os.listdir(tr_ab_path))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Oa2mxylBDVM5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":9,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"ab42b7ca-dcce-4754-ca25-bb7061c88483"},"outputs":[{"output_type":"stream","name":"stdout","text":["[60, 61]\n","[48, 48]\n","[194, 194]\n"]}],"source":["# test 사이즈 : 전체 이미지의 20%\n","te_data_num = [round(len(os.listdir(tr_n_path))*0.2), round(len(os.listdir(tr_ab_path))*0.2)]\n","print(te_data_num)\n","\n","# validation 사이즈 : test를 제외한 나머지 중에서 20%\n","val_data_num = [ round((len(os.listdir(tr_n_path))-te_data_num[0])*0.2) , round((len(os.listdir(tr_n_path))-te_data_num[1])*0.2) ]\n","print(val_data_num)\n","\n","# train 사이즈\n","train_data_num = [len(os.listdir(tr_n_path)) - te_data_num[0] - val_data_num[0],\n","                  len(os.listdir(tr_ab_path))- te_data_num[1] - val_data_num[1]]\n","print(train_data_num)"]},{"cell_type":"code","source":["train_n_set = os.listdir(tr_n_path)\n","train_ab_set = os.listdir(tr_ab_path)"],"metadata":{"id":"VitSC3is_VGl","executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":8,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["#### 2) test 셋 추출"],"metadata":{"id":"RmRhrViWRXgL"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"TSwovHr2Fon1","executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":8,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["random.seed(2023)\n","te_n_path = '/content/drive/MyDrive/Datasets/Car_Image_test/normal/'\n","te_ab_path = '/content/drive/MyDrive/Datasets/Car_Image_test/abnormal/'\n","\n","test_n_set = random.sample(os.listdir(tr_n_path), k = round(len(os.listdir(tr_n_path))*0.2))\n","test_ab_set = random.sample(os.listdir(tr_ab_path), k = round(len(os.listdir(tr_ab_path))*0.2))\n","for file in test_n_set :\n","    now_path = tr_n_path + file\n","    shutil.move(now_path, te_n_path)\n","\n","for file in test_ab_set :\n","    now_path = tr_ab_path + file\n","    shutil.move(now_path, te_ab_path)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"AImO1ujiI2IY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":7,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"2e088472-163a-470b-e7bc-49ce1b6c1b84"},"outputs":[{"output_type":"stream","name":"stdout","text":["60 61\n","242 242\n"]}],"source":["# 추출 후 이미지 갯수 확인\n","\n","print(len(os.listdir(te_n_path)), len(os.listdir(te_ab_path)))\n","print(len(os.listdir(tr_n_path)) , len(os.listdir(tr_ab_path)))"]},{"cell_type":"markdown","source":["#### 3) validation 셋 추출"],"metadata":{"id":"2V4mh3hxRpR2"}},{"cell_type":"code","execution_count":15,"metadata":{"id":"zXYmEdCjAEDu","executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":6,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["random.seed(2023)\n","val_n_path = '/content/drive/MyDrive/Datasets/Car_Image_val/normal/'\n","val_ab_path = '/content/drive/MyDrive/Datasets/Car_Image_val/abnormal/'\n","\n","val_n_set = random.sample(os.listdir(tr_n_path), k = round(len(os.listdir(tr_n_path))*0.2))\n","val_ab_set = random.sample(os.listdir(tr_ab_path), k = round(len(os.listdir(tr_ab_path))*0.2))\n","\n","for file in val_n_set :\n","    now_path = tr_n_path + file\n","    shutil.move(now_path, val_n_path)\n","\n","for file in val_ab_set :\n","    now_path = tr_ab_path + file\n","    shutil.move(now_path, val_ab_path)\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"yIT85iSdM4U-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376151703,"user_tz":-540,"elapsed":6,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"8f08874c-745f-48f8-8dd5-a18ff083671a"},"outputs":[{"output_type":"stream","name":"stdout","text":["48 48\n","194 194\n"]}],"source":["# 추출 후 이미지 갯수 확인\n","len(val_n_set), len(val_ab_set)\n","\n","print(len(os.listdir(val_n_path)), len(os.listdir(val_ab_path)))\n","print(len(os.listdir(tr_n_path)) , len(os.listdir(tr_ab_path)))"]},{"cell_type":"code","source":["print(len(os.listdir(te_n_path)), len(os.listdir(te_ab_path)))\n","print(len(os.listdir(val_n_path)), len(os.listdir(val_ab_path)))\n","print(len(os.listdir(tr_n_path)) , len(os.listdir(tr_ab_path)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwI01TuQAEFD","executionInfo":{"status":"ok","timestamp":1679376151704,"user_tz":-540,"elapsed":6,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"7c269ff6-087a-42c7-f100-e7f502a79072"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["60 61\n","48 48\n","194 194\n"]}]},{"cell_type":"markdown","metadata":{"id":"haSO004sgyyu"},"source":["### (2) 데이터 복사 및 이동\n","- **세부요구사항**\n","    - 분할된 데이터를 복사 이동합니다.\n","        - 새로운 폴더에 저장하는 데이터로 \"3.모델링I\"에서 사용합니다.\n","        - 기존 폴더는 \"4.모델링II > (1) Data Augmentation\"에서 사용합니다.\n","    - Training set | Validation set | Test set의 데이터를 **새로운 폴더**에 복사하세요.\n","        - 새로운 폴더 명\n","            * copy_images/trainset\n","            * copy_images/validset\n","            * copy_images/testset\n","        - 새로운 폴더에는 normal, abnormal 파일 모두를 복사합니다. \n","            * 파일을 구분하기 위해 abnormal 파일들은 파일명 앞에 접두사 'ab_'를 붙입시다.\n","        - os, shutil 모듈을 활용하세요."]},{"cell_type":"markdown","source":["#### 1) abnormal 파일 복사"],"metadata":{"id":"3UbNfTY4kOSZ"}},{"cell_type":"markdown","source":["* 복사하기 : shutil.copytree()"],"metadata":{"id":"zhkKqLfTkjGI"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"aTMVxJJJya98","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1679376157253,"user_tz":-540,"elapsed":5554,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"281eb077-809c-4c03-fb78-fdbf28065b60"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Datasets/copy_images/testset/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["shutil.copytree('/content/drive/MyDrive/Datasets/Car_Image_train/abnormal/', dataset_path+'copy_images/trainset/')\n","shutil.copytree('/content/drive/MyDrive/Datasets/Car_Image_val/abnormal/', dataset_path+'copy_images/validset/')\n","shutil.copytree('/content/drive/MyDrive/Datasets/Car_Image_test/abnormal/', dataset_path+'copy_images/testset/')"]},{"cell_type":"markdown","source":["* abnormal 이미지 이름의 접두어 \"ab_\" 붙이기 : os.rename"],"metadata":{"id":"mU0T-ypHkV6D"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"Cv6gafRyz6ul","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376158511,"user_tz":-540,"elapsed":1263,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"7a3a47fa-5073-4b6c-fc64-5100f202c0c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["DALLíñE 2023-03-10 18.51.26 - scratched car.png\n","DALLíñE 2023-03-10 18.51.29 - scratched car.png\n","DALLíñE 2023-03-10 18.51.32 - scratched car.png\n","DALLíñE 2023-03-10 18.53.06 - scratched car.png\n","DALLíñE 2023-03-10 18.53.08 - scratched car.png\n","DALLíñE 2023-03-10 18.53.58 - slightly damaged car.png\n","DALLíñE 2023-03-10 18.54.17 - slightly damaged car.png\n","DALLíñE 2023-03-10 18.54.19 - slightly damaged car.png\n","DALLíñE 2023-03-10 18.54.24 - slightly damaged car.png\n","DALLíñE 2023-03-10 22.04.36 - scratched car.png\n","DALLíñE 2023-03-10 22.04.39 - scratched car.png\n","DALLíñE 2023-03-10 22.04.42 - scratched car.png\n","DALLíñE 2023-03-10 22.37.56 - photo of a part of car.png\n","DALLíñE 2023-03-10 23.28.30 - photo of a part of car without blemish.png\n","DALLíñE 2023-03-10 23.34.52 - photo of a part of car without blemish.png\n","DALLíñE 2023-03-10 23.39.41 - photo of a part of car without blemish.png\n","DALLíñE 2023-03-10 23.55.00 - a part of car without blemish.png\n","DALLíñE 2023-03-10 23.55.04 - a part of car without blemish.png\n","DALLíñE 2023-03-10 23.56.50 - a part of car without blemish.png\n","DALLíñE 2023-03-10 23.57.31 - a part of car without blemish.png\n","DALLíñE 2023-03-10 23.58.03 - a part of car without blemish.png\n","DALLíñE 2023-03-10 23.58.06 - a part of car without blemish.png\n","DALLíñE 2023-03-11 00.03.13 - a part of car without blemish.png\n","DALLíñE 2023-03-11 00.12.34 - a part of car without scrach.png\n","DALLíñE 2023-03-11 00.14.25 - a part of car without scratch.png\n","DALLíñE 2023-03-11 00.14.28 - a part of car without scratch.png\n","DALLíñE 2023-03-11 00.16.36 - photo of a part of car without blemish.png\n","DALLíñE 2023-03-11 01.21.50 - scratched car.png\n","DALLíñE 2023-03-11 01.22.25 - scratched car.png\n","DALLíñE 2023-03-11 01.22.27 - scratched car.png\n","DALLíñE 2023-03-11 01.23.26 - slightly damaged car.png\n","DALLíñE 2023-03-11 01.23.29 - slightly damaged car.png\n","DALLíñE 2023-03-11 01.23.56 - slightly damaged car.png\n","DALLíñE 2023-03-11 01.23.58 - slightly damaged car.png\n","DALLíñE 2023-03-11 01.25.20 - slightly scratched car.png\n","DALLíñE 2023-03-11 01.25.26 - slightly scratched car.png\n","DALLíñE 2023-03-11 01.29.03 - scratched car.png\n","DALLíñE 2023-03-11 01.29.04 - scratched car.png\n","DALLíñE 2023-03-11 01.29.06 - scratched car.png\n","DALLíñE 2023-03-11 01.29.08 - scratched car.png\n","DALLíñE 2023-03-11 01.29.55 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.30.00 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.30.47 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.30.49 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.31.20 - slightly damaged car.png\n","DALLíñE 2023-03-11 01.31.29 - slightly damaged car.png\n","DALLíñE 2023-03-11 01.32.19 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.32.21 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.32.23 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.32.25 - a little bit scratched car.png\n","DALLíñE 2023-03-11 01.32.58 - slightly damaged car.png\n","DALLíñE 2023-03-11 01.33.00 - slightly damaged car.png\n","DALLíñE 2023-03-11 14.48.31 - dents of a car.png\n","DALLíñE 2023-03-11 14.48.33 - dents of a car.png\n","DALLíñE 2023-03-11 14.49.06 - dents of a car.png\n","DALLíñE 2023-03-11 14.49.32 - dents of a car.png\n","DALLíñE 2023-03-11 14.49.59 - dents of a car.png\n","DALLíñE 2023-03-11 14.50.01 - dents of a car.png\n","DALLíñE 2023-03-11 14.50.36 - dents of a car.png\n","DALLíñE 2023-03-11 14.50.39 - dents of a car.png\n","DALLíñE 2023-03-11 14.51.19 - dents of a car.png\n","DALLíñE 2023-03-11 14.52.54 - dents of a car.png\n","DALLíñE 2023-03-11 14.52.56 - dents of a car.png\n","DALLíñE 2023-03-11 14.53.25 - dents of a car.png\n","DALLíñE 2023-03-11 14.53.49 - dents of a car.png\n","DALLíñE 2023-03-11 14.53.52 - dents of a car.png\n","DALLíñE 2023-03-11 14.54.38 - dents of a car.png\n","DALLíñE 2023-03-11 14.54.42 - dents of a car.png\n","DALLíñE 2023-03-11 14.55.03 - dents of a car.png\n","DALLíñE 2023-03-11 14.55.32 - dents of a car.png\n","DALLíñE 2023-03-11 14.55.34 - dents of a car.png\n","DALLíñE 2023-03-11 14.56.30 - dents of a car.png\n","DALLíñE 2023-03-11 14.56.57 - dents of a car.png\n","DALLíñE 2023-03-11 14.57.01 - dents of a car.png\n","DALLíñE 2023-03-11 14.57.26 - dents of a car.png\n","DALLíñE 2023-03-11 14.57.28 - dents of a car.png\n","DALLíñE 2023-03-11 14.58.17 - dents of a car.png\n","DALLíñE 2023-03-11 14.58.40 - dents of a car.png\n","DALLíñE 2023-03-11 14.59.08 - dents of a car.png\n","DALLíñE 2023-03-11 14.59.32 - dents of a car.png\n","DALLíñE 2023-03-11 15.00.14 - dents of a car.png\n","DALLíñE 2023-03-11 15.00.21 - dents of a car.png\n","DALLíñE 2023-03-11 15.00.43 - dents of a car.png\n","DALLíñE 2023-03-11 15.01.15 - dents of a car.png\n","DALLíñE 2023-03-11 15.01.57 - dents of a car.png\n","DALLíñE 2023-03-11 15.02.00 - dents of a car.png\n","DALLíñE 2023-03-11 15.03.37 - dents of a car.png\n","DALLíñE 2023-03-11 15.04.50 - dents of a car.png\n","DALLíñE 2023-03-11 15.05.16 - dents of a car.png\n","DALLíñE 2023-03-11 15.05.23 - dents of a car.png\n","DALLíñE 2023-03-11 15.05.44 - dents of a car.png\n","DALLíñE 2023-03-11 15.05.50 - dents of a car.png\n","DALLíñE 2023-03-11 15.06.13 - dents of a car.png\n","DALLíñE 2023-03-11 15.06.15 - dents of a car.png\n","DALLíñE 2023-03-11 15.06.35 - dents of a car.png\n","DALLíñE 2023-03-11 15.07.12 - dents of a car.png\n","DALLíñE 2023-03-11 15.07.39 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.00 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.05 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.24 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.28 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.48 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.50 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.52 - dents of a car.png\n","DALLíñE 2023-03-11 15.08.54 - dents of a car.png\n","DALLíñE 2023-03-11 15.09.38 - dents of a car.png\n","DALLíñE 2023-03-11 15.09.54 - dents of a car.png\n","DALLíñE 2023-03-11 15.09.59 - dents of a car.png\n","DALLíñE 2023-03-11 15.10.01 - dents of a car.png\n","DALLíñE 2023-03-11 15.10.43 - dents of a car.png\n","DALLíñE 2023-03-11 15.11.06 - dents of a car.png\n","DALLíñE 2023-03-11 15.11.09 - dents of a car.png\n","DALLíñE 2023-03-11 15.12.06 - dents of a car.png\n","DALLíñE 2023-03-11 15.12.25 - dents of a car.png\n","DALLíñE 2023-03-11 15.12.28 - dents of a car.png\n","DALLíñE 2023-03-11 15.12.31 - dents of a car.png\n","DALLíñE 2023-03-11 15.13.25 - dents of a car.png\n","DALLíñE 2023-03-11 15.13.45 - dents of a car.png\n","DALLíñE 2023-03-11 15.13.48 - dents of a car.png\n","DALLíñE 2023-03-11 17.10.24 - scratched car.png\n","DALLíñE 2023-03-11 17.10.30 - scratched car.png\n","DALLíñE 2023-03-11 17.14.15 - scratched car.png\n","DALLíñE 2023-03-11 17.14.17 - scratched car.png\n","DALLíñE 2023-03-11 17.14.21 - scratched car.png\n","DALLíñE 2023-03-11 17.15.16 - scratched car.png\n","DALLíñE 2023-03-11 17.15.20 - scratched car.png\n","DALLíñE 2023-03-11 17.15.58 - damaged car.png\n","DALLíñE 2023-03-11 17.16.21 - damaged car.png\n","DALLíñE 2023-03-11 17.16.24 - damaged car.png\n","DALLíñE 2023-03-11 17.16.50 - damaged car.png\n","DALLíñE 2023-03-11 17.16.53 - damaged car.png\n","DALLíñE 2023-03-11 17.17.27 - scratched car.png\n","DALLíñE 2023-03-11 17.17.32 - scratched car.png\n","DALLíñE 2023-03-11 17.17.37 - scratched car.png\n","DALLíñE 2023-03-11 17.18.02 - scratched car.png\n","DALLíñE 2023-03-11 17.18.25 - scratched car.png\n","DALLíñE 2023-03-11 17.18.29 - scratched car.png\n","DALLíñE 2023-03-11 17.18.31 - scratched car.png\n","DALLíñE 2023-03-11 17.18.46 - scratched car.png\n","DALLíñE 2023-03-11 17.18.47 - scratched car.png\n","DALLíñE 2023-03-11 17.18.49 - scratched car.png\n","DALLíñE 2023-03-11 17.18.51 - scratched car.png\n","DALLíñE 2023-03-11 17.22.16 - scratched car.png\n","DALLíñE 2023-03-11 17.23.56 - slightly dented car.png\n","DALLíñE 2023-03-11 17.23.58 - slightly dented car.png\n","DALLíñE 2023-03-11 17.24.00 - slightly dented car.png\n","DALLíñE 2023-03-11 17.25.10 - slightly dented car.png\n","DALLíñE 2023-03-11 17.25.12 - slightly dented car.png\n","DALLíñE 2023-03-11 17.25.14 - slightly dented car.png\n","DALLíñE 2023-03-11 17.26.11 - slightly dented car.png\n","DALLíñE 2023-03-11 17.26.14 - slightly dented car.png\n","DALLíñE 2023-03-11 17.26.17 - slightly dented car.png\n","DALLíñE 2023-03-11 17.27.04 - slightly dented car.png\n","DALLíñE 2023-03-11 17.27.08 - slightly dented car.png\n","DALLíñE 2023-03-11 17.27.56 - slightly dented car.png\n","DALLíñE 2023-03-11 17.27.58 - slightly dented car.png\n","DALLíñE 2023-03-11 17.28.25 - slightly dented car.png\n","DALLíñE 2023-03-11 17.28.29 - slightly dented car.png\n","DALLíñE 2023-03-11 17.28.31 - slightly dented car.png\n","DALLíñE 2023-03-11 17.28.33 - slightly dented car.png\n","DALLíñE 2023-03-11 17.29.49 - slightly dented car.png\n","DALLíñE 2023-03-11 17.29.59 - slightly dented car.png\n","DALLíñE 2023-03-11 17.30.01 - slightly dented car.png\n","DALLíñE 2023-03-11 17.30.04 - slightly dented car.png\n","DALLíñE 2023-03-11 17.31.37 - slightly dented car.png\n","DALLíñE 2023-03-11 17.31.41 - slightly dented car.png\n","DALLíñE 2023-03-11 17.31.55 - slightly dented car.png\n","DALLíñE 2023-03-11 18.41.26 - slightly dented car.png\n","DALLíñE 2023-03-11 18.41.32 - slightly dented car.png\n","DALLíñE 2023-03-11 18.41.54 - slightly dented car.png\n","DALLíñE 2023-03-11 18.41.56 - slightly dented car.png\n","DALLíñE 2023-03-11 18.42.01 - slightly dented car.png\n","DALLíñE 2023-03-11 18.42.06 - slightly dented car.png\n","DALLíñE 2023-03-11 18.42.27 - slightly dented car.png\n","DALLíñE 2023-03-11 18.42.29 - slightly dented car.png\n","DALLíñE 2023-03-11 18.42.32 - slightly dented car.png\n","DALLíñE 2023-03-11 18.42.57 - slightly dented car.png\n","DALLíñE 2023-03-11 18.42.59 - slightly dented car.png\n","DALLíñE 2023-03-11 18.43.01 - slightly dented car.png\n","DALLíñE 2023-03-11 18.43.03 - slightly dented car.png\n","DALLíñE 2023-03-11 18.43.29 - slightly dented car.png\n","DALLíñE 2023-03-11 18.43.31 - slightly dented car.png\n","DALLíñE 2023-03-11 18.43.34 - slightly dented car.png\n","DALLíñE 2023-03-11 18.44.13 - slightly dented car.png\n","DALLíñE 2023-03-11 18.44.30 - slightly dented car.png\n","DALLíñE 2023-03-11 18.44.56 - slightly dented car.png\n","DALLíñE 2023-03-11 18.45.00 - slightly dented car.png\n","DALLíñE 2023-03-11 18.45.06 - slightly dented car.png\n","DALLíñE 2023-03-11 18.45.28 - slightly dented car.png\n","DALLíñE 2023-03-11 18.45.32 - slightly dented car.png\n","DALLíñE 2023-03-11 18.45.52 - scratched car.png\n","DALLíñE 2023-03-11 18.45.56 - scratched car.png\n","DALLíñE 2023-03-11 18.46.00 - scratched car.png\n","DALLíñE 2023-03-11 18.46.24 - scratched car.png\n"]}],"source":["trainset_path = '/content/drive/MyDrive/Datasets/copy_images/trainset/'\n","validset_path = '/content/drive/MyDrive/Datasets/copy_images/validset/'\n","testset_path = '/content/drive/MyDrive/Datasets/copy_images/testset/'\n","\n","for file in os.listdir(trainset_path) :\n","    print(file)\n","    os.rename(trainset_path + file, trainset_path + 'ab_' + file)\n","\n","for file in os.listdir(validset_path) :\n","    os.rename(validset_path + file, validset_path + 'ab_' + file)\n","\n","for file in os.listdir(testset_path) :\n","    os.rename(testset_path + file, testset_path + 'ab_' + file)\n","\n"]},{"cell_type":"markdown","source":["#### 2) normal 파일 복사"],"metadata":{"id":"Nk6xITmTksyK"}},{"cell_type":"code","execution_count":20,"metadata":{"id":"Vw3DmdTS17RM","executionInfo":{"status":"ok","timestamp":1679376163072,"user_tz":-540,"elapsed":4566,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["nomal_train_path = '/content/drive/MyDrive/Datasets/Car_Image_train/normal/'\n","nomal_val_path = '/content/drive/MyDrive/Datasets/Car_Image_val/normal/'\n","nomal_test_path = '/content/drive/MyDrive/Datasets/Car_Image_test/normal/'\n","\n","for file in os.listdir(nomal_train_path) : \n","    shutil.copyfile(nomal_train_path+file , dataset_path+'copy_images/trainset/'+file)\n","\n","for file in os.listdir(nomal_val_path) : \n","    shutil.copyfile(nomal_val_path+file , dataset_path+'copy_images/validset/'+file)\n","\n","for file in os.listdir(nomal_test_path) : \n","    shutil.copyfile(nomal_test_path+file , dataset_path+'copy_images/testset/'+file)"]},{"cell_type":"markdown","source":["* 데이터 갯수 조회"],"metadata":{"id":"xzEXHZrqkz88"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"ugNprP9d-Gti","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376163072,"user_tz":-540,"elapsed":5,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"b99d8931-2d98-49ee-b73d-81a36eec5236"},"outputs":[{"output_type":"stream","name":"stdout","text":["388\n","96\n","121\n"]}],"source":["print(len(os.listdir(dataset_path+'copy_images/trainset/')))\n","print(len(os.listdir(dataset_path+'copy_images/validset/')))\n","print(len(os.listdir(dataset_path+'copy_images/testset/')))"]},{"cell_type":"code","source":["print(len(os.listdir(tr_n_path)) + len(os.listdir(tr_ab_path)))\n","print(len(os.listdir(val_n_path)) + len(os.listdir(val_ab_path)))\n","print(len(os.listdir(te_n_path)) + len(os.listdir(te_ab_path)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmo20zyJa0Wc","executionInfo":{"status":"ok","timestamp":1679376163072,"user_tz":-540,"elapsed":3,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"26770ee8-c9c2-4a7c-e008-ec1c21cd05df"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["388\n","96\n","121\n"]}]},{"cell_type":"markdown","metadata":{"id":"VfYDW1Pj7ZdU"},"source":["## 3.모델링 I\n","* **세부요구사항**\n","    * 모델링을 위한 데이터 구조 만들기\n","        * x : 이미지를 array로 변환합니다.\n","        * y : 이미지 갯수만큼 normal - 0, abnormal - 1 로 array를 만듭니다.\n","    * 모델을 최소 3개 이상 만들고 성능을 비교합니다.\n","        * 모델 학습 과정에 알맞은 보조 지표를 사용하세요.\n","        * 전처리 과정에서 생성한 Validation set을 적절하게 사용하세요.\n","        * Early Stopping을 반드시 사용하세요.\n","            * 최적의 가중치를 모델에 적용하세요."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Rg553KIvxE6W","executionInfo":{"status":"ok","timestamp":1679376166200,"user_tz":-540,"elapsed":3130,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import classification_report, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"wIfqg6e0xE6A"},"source":["### (1) X : image to array\n","- **세부요구사항**\n","    * 모델링을 위해서는 np.array 형태로 데이터셋을 만들어야 합니다.\n","    * Training set / Validation set / Test set의 X는 이미지 형태로 되어있습니다. \n","    * 이미지 파일을 불러와 train, valid, test 각각 array 형태로 변환해 봅시다.\n","        * 각 폴더로 부터 이미지 목록을 만들고\n","        * 이미지 한장씩 적절한 크기로 로딩하여 (keras.utils.load_img)\n","            * 이미지가 너무 크면 학습시간이 많이 걸리고, 메모리 부족현상이 발생될 수 있습니다.\n","            * 이미지 크기를 280 * 280 * 3 이내의 크기를 설정하여 로딩하시오.\n","            * array로 변환 (keras.utils.img_to_array, np.expand_dims)\n","        * 데이터셋에 추가합니다.(데이터셋도 array)"]},{"cell_type":"markdown","source":["#### 1) 이미지 목록 만들기\n","* train, validation, test 폴더로 부터 이미지 목록을 생성합니다."],"metadata":{"id":"FovkIeSDT367"}},{"cell_type":"code","execution_count":24,"metadata":{"id":"X022f0QMxE6W","executionInfo":{"status":"ok","timestamp":1679376166200,"user_tz":-540,"elapsed":7,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["# 이미지 목록 저장\n","img_train_list = os.listdir(dataset_path+'copy_images/trainset/')\n","img_valid_list = os.listdir(dataset_path+'copy_images/validset/')\n","img_test_list = os.listdir(dataset_path+'copy_images/testset/')"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"rgvW_LQfxE6X","executionInfo":{"status":"ok","timestamp":1679376166200,"user_tz":-540,"elapsed":6,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["# 메모리, 처리시간을 위해서 이미지 크기 조정\n","img_size = 224 ## 사이즈 조정 가능"]},{"cell_type":"code","source":["from keras.utils import load_img, img_to_array\n","\n","# x_train\n","x_train = []\n","train_path = dataset_path+'copy_images/trainset/'\n","for file in img_train_list :\n","    img_path = train_path + file\n","    img = load_img(img_path, target_size=(img_size, img_size))\n","    img_array = img_to_array(img)\n","    x_train.append(img_array)\n","x_train = np.array(x_train)\n","\n","# x_valid\n","x_valid = []\n","val_path = dataset_path+'copy_images/validset/'\n","for file in img_valid_list :\n","    img_path = val_path + file\n","    img = load_img(img_path, target_size=(img_size, img_size))\n","    img_array = img_to_array(img)\n","    x_valid.append(img_array)\n","x_valid = np.array(x_valid)\n","\n","# x_test\n","\n","x_test = []\n","test_path = dataset_path+'copy_images/testset/'\n","for file in img_test_list :\n","    img_path = test_path + file\n","    img = load_img(img_path, target_size=(img_size, img_size))\n","    img_array = img_to_array(img)\n","    x_test.append(img_array)\n","x_test = np.array(x_test)"],"metadata":{"id":"COSOP0mrb7lp","executionInfo":{"status":"ok","timestamp":1679376186652,"user_tz":-540,"elapsed":20458,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["x_train.shape, x_valid.shape, x_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxTJUv8g552u","executionInfo":{"status":"ok","timestamp":1679376186653,"user_tz":-540,"elapsed":22,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"f7f7e7b0-cfc2-446d-f628-514231f0ca53"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((388, 224, 224, 3), (96, 224, 224, 3), (121, 224, 224, 3))"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":[],"metadata":{"id":"gb-xpliPm5ov","executionInfo":{"status":"ok","timestamp":1679376186653,"user_tz":-540,"elapsed":18,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 이미지들을 배열 데이터셋으로 만들기"],"metadata":{"id":"LSt88mjPV33u"}},{"cell_type":"code","execution_count":28,"metadata":{"id":"rhEdBiKfxE6Y","executionInfo":{"status":"ok","timestamp":1679376186654,"user_tz":-540,"elapsed":18,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["###############################################"]},{"cell_type":"markdown","metadata":{"id":"doUM37LxxE6Z"},"source":["### (2) y : 클래스 만들기\n","- **세부요구사항**\n","    - Training set / Validation set / Test set의 y를 생성합니다.\n","        - 각각 normal, abnormal 데이터의 갯수를 다시 확인하고\n","        - normal을 0, abnormal을 1로 지정합니다."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"8nl1Uv9UxE6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376186654,"user_tz":-540,"elapsed":18,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"375f08d7-8d4a-4940-d4db-1bfe0ae9f590"},"outputs":[{"output_type":"stream","name":"stdout","text":["388\n","194\n","---\n","96\n","48\n","---\n","121\n","61\n"]}],"source":["# 데이터 갯수 확인\n","print( len(img_train_list) )\n","print( len([val for val in img_train_list if val.startswith('ab_')]) )\n","print('---')\n","print( len(img_valid_list) )\n","print( len([val for val in img_valid_list if val.startswith('ab_')]) )\n","print('---')\n","print( len(img_test_list) )\n","print( len([val for val in img_test_list if val.startswith('ab_')]) )"]},{"cell_type":"markdown","source":["* y_train, y_valid, y_test 만들기\n","    * normal, abnormal 데이터의 갯수를 다시 확인하고 normal을 0, abnormal을 1로 지정합니다."],"metadata":{"id":"HIfaCLlNn04C"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"YVrPQdhTxE6b","executionInfo":{"status":"ok","timestamp":1679376186654,"user_tz":-540,"elapsed":14,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["y_train, y_valid, y_test = [0] * x_train.shape[0], [0] * x_valid.shape[0], [0] * x_test.shape[0]\n","\n","# y_train\n","for idx, file in enumerate(img_train_list):\n","    if file.startswith('ab_') : y_train[idx] = 1\n","y_train = np.array(y_train)\n","# y_valid\n","for idx, file in enumerate(img_valid_list):\n","    if file.startswith('ab_') : y_valid[idx] = 1\n","y_valid = np.array(y_valid)\n","# \n","for idx, file in enumerate(img_test_list):\n","    if file.startswith('ab_') : y_test[idx] = 1\n","y_test = np.array(y_test)"]},{"cell_type":"code","source":["x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-sk40XcG1Y_","executionInfo":{"status":"ok","timestamp":1679376186654,"user_tz":-540,"elapsed":13,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"0e34d0e2-5037-42c5-d8d6-29ee63a6df7d"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((388, 224, 224, 3),\n"," (388,),\n"," (96, 224, 224, 3),\n"," (96,),\n"," (121, 224, 224, 3),\n"," (121,))"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["print(y_valid)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TXhRtbXNAGe","executionInfo":{"status":"ok","timestamp":1679376186654,"user_tz":-540,"elapsed":11,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"0120382c-43ed-40cd-99f1-6b136ec0bc85"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"]}]},{"cell_type":"markdown","source":["## Min-Max Scaling"],"metadata":{"id":"SaJBCn0cKm77"}},{"cell_type":"code","source":["max_x = x_train.max() \n","min_x = x_train.min()\n","\n","x_train = (x_train - min_x)/(max_x - min_x)\n","x_valid = (x_valid - min_x)/(max_x - min_x)\n","x_test = (x_test - min_x)/(max_x - min_x)"],"metadata":{"id":"hRCw-Rf2KsYt","executionInfo":{"status":"ok","timestamp":1679376186655,"user_tz":-540,"elapsed":10,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## One-Hot Encoding"],"metadata":{"id":"OEi4UTNkLRgT"}},{"cell_type":"code","source":["from keras.utils import to_categorical\n","\n","class_n = len(np.unique(y_train))\n","y_train = to_categorical(y_train, class_n)\n","y_valid = to_categorical(y_valid, class_n)\n","y_test = to_categorical(y_test, class_n)"],"metadata":{"id":"8MlVyE0aLUqi","executionInfo":{"status":"ok","timestamp":1679376186655,"user_tz":-540,"elapsed":10,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nK8nm95zyRKn","executionInfo":{"status":"ok","timestamp":1679376186655,"user_tz":-540,"elapsed":9,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"e5307dd8-a778-46a3-88d3-4ad2009dac7e"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(388, 2)"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"Z586wXFu7ZgT"},"source":["### (3) 모델1\n","- **세부요구사항**\n","    - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 validation_data로 validation set을 사용하시오.\n","    - 반드시 Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["#### 1) 구조 설계"],"metadata":{"id":"NIvIO6RKa0mp"}},{"cell_type":"markdown","source":["## AlexNet 미니버전"],"metadata":{"id":"7TtIIz6XJQ5E"}},{"cell_type":"markdown","source":["# 3. 미니버젼 예시 :\n","    1. 처음 컨볼루션은 스트라이드1, 필터사이즈 5*5, 필터수 동일\n","    2. 맥스풀들은 전부 2*2\n","    3. 컨볼루션은 아래의 3X3 same. 13x13x384 까지(아래 맨 처음까지만)\n","    4. 3번에 이어서 바로 플래튼\n","    5. 덴스레이어는 순서대로 노드 384개, 192개 (2개만)\n","    6. 적절한 아웃풋레이어\n","\n"],"metadata":{"id":"uWKa71RNsnNz"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","from keras.layers import Input, Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization\n","\n","from sklearn.metrics import confusion_matrix, classification_report"],"metadata":{"id":"CF8E_nfVtueH","executionInfo":{"status":"ok","timestamp":1679376289619,"user_tz":-540,"elapsed":828,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"DHM91_bha3Kc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHnFVZuKa42f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679366203061,"user_tz":-540,"elapsed":4912,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"4651e97c-f5fa-43cd-8115-f2768c484a13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 227, 227, 3)]     0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 223, 223, 96)      7296      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 111, 111, 96)     0         \n"," )                                                               \n","                                                                 \n"," batch_normalization (BatchN  (None, 111, 111, 96)     384       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 111, 111, 256)     614656    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 55, 55, 256)      0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 55, 55, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 55, 55, 384)       885120    \n","                                                                 \n"," flatten (Flatten)           (None, 1161600)           0         \n","                                                                 \n"," dense (Dense)               (None, 384)               446054784 \n","                                                                 \n"," dense_1 (Dense)             (None, 192)               73920     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 192)              768       \n"," hNormalization)                                                 \n","                                                                 \n"," dropout (Dropout)           (None, 192)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 386       \n","                                                                 \n","=================================================================\n","Total params: 447,638,338\n","Trainable params: 447,637,250\n","Non-trainable params: 1,088\n","_________________________________________________________________\n"]}],"source":["# 1. 세션 클리어\n","keras.backend.clear_session()\n","\n","# 2. 모델 사슬처럼 엮기\n","il = Input(shape = (227, 227, 3))\n","hl = Conv2D( filters = 96, kernel_size = (5, 5), strides = (1, 1), activation = 'relu')(il)\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 256, kernel_size = (5, 5), padding = 'same', activation = 'relu')(hl)\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 384, kernel_size = (3, 3), padding = 'same', activation = 'relu')(hl)\n","hl = Flatten()(hl)\n","\n","hl = Dense(384, activation = 'relu')(hl)\n","hl = Dense(192, activation = 'relu')(hl)\n","hl = BatchNormalization()(hl)\n","hl = Dropout(0.25)(hl)\n","\n","ol = Dense(2, activation = 'sigmoid')(hl)\n","\n","model_mini_alexnet = keras.models.Model(il, ol)\n","\n","\n","#  4. 모델 컴파일\n","model_mini_alexnet.compile(loss = 'binary_crossentropy', metrics = ['accuracy'],\n","              optimizer = 'adam')\n","# 모델 요약\n","model_mini_alexnet.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BnrTSupKa42f"},"outputs":[],"source":["from keras.callbacks import EarlyStopping\n","\n","es = EarlyStopping(monitor = 'val_loss',\n","                   min_delta = 0,\n","                   verbose = 1,\n","                   patience = 7,\n","                   restore_best_weights = True\n","                   )"]},{"cell_type":"code","source":["model_mini_alexnet.fit(x_train, y_train, callbacks = [es], epochs = 1000, validation_data = (x_valid, y_valid), verbose = 1, batch_size=32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msp3WVhAzcGi","executionInfo":{"status":"ok","timestamp":1679366332350,"user_tz":-540,"elapsed":55677,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"c6515d69-e540-494c-b78b-0264a3235b62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","13/13 [==============================] - 9s 749ms/step - loss: 0.4380 - accuracy: 0.8170 - val_loss: 4.2972 - val_accuracy: 0.4896\n","Epoch 2/1000\n","13/13 [==============================] - 6s 460ms/step - loss: 0.3558 - accuracy: 0.8686 - val_loss: 0.9985 - val_accuracy: 0.4896\n","Epoch 3/1000\n","13/13 [==============================] - 4s 306ms/step - loss: 0.3133 - accuracy: 0.8943 - val_loss: 1.1055 - val_accuracy: 0.5312\n","Epoch 4/1000\n","13/13 [==============================] - 6s 461ms/step - loss: 0.2717 - accuracy: 0.8995 - val_loss: 0.9094 - val_accuracy: 0.5312\n","Epoch 5/1000\n","13/13 [==============================] - 4s 308ms/step - loss: 0.2340 - accuracy: 0.9227 - val_loss: 1.4769 - val_accuracy: 0.5000\n","Epoch 6/1000\n","13/13 [==============================] - 4s 309ms/step - loss: 0.1971 - accuracy: 0.9330 - val_loss: 2.1251 - val_accuracy: 0.5000\n","Epoch 7/1000\n","13/13 [==============================] - 4s 310ms/step - loss: 0.1346 - accuracy: 0.9613 - val_loss: 3.0921 - val_accuracy: 0.5000\n","Epoch 8/1000\n","13/13 [==============================] - 4s 311ms/step - loss: 0.0818 - accuracy: 0.9845 - val_loss: 2.6573 - val_accuracy: 0.5000\n","Epoch 9/1000\n","13/13 [==============================] - 4s 312ms/step - loss: 0.0798 - accuracy: 0.9768 - val_loss: 2.5037 - val_accuracy: 0.5000\n","Epoch 10/1000\n","13/13 [==============================] - 4s 313ms/step - loss: 0.1109 - accuracy: 0.9665 - val_loss: 3.0836 - val_accuracy: 0.5000\n","Epoch 11/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9768Restoring model weights from the end of the best epoch: 4.\n","13/13 [==============================] - 6s 446ms/step - loss: 0.0837 - accuracy: 0.9768 - val_loss: 1.4418 - val_accuracy: 0.5208\n","Epoch 11: early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fab481a0b20>"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["#### 3) test set으로 예측하고 평가하기\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"zage6-Z0a6DX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xkFFlFdbBZb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679366346817,"user_tz":-540,"elapsed":2449,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"a14e3c01-e681-4ee8-c622-44132892f0f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 2s 587ms/step\n"]}],"source":["y_pred = model_mini_alexnet.predict(x_test)"]},{"cell_type":"code","source":["single_y_pred = y_pred.argmax(axis=1)\n","single_y_test = y_test.argmax(axis=1)"],"metadata":{"id":"O5X0Ko521zjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["confusion_matrix(single_y_test, single_y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNt60bo511GX","executionInfo":{"status":"ok","timestamp":1679366351875,"user_tz":-540,"elapsed":4,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"70739698-d94b-4942-f2df-015e47a0e223"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3, 57],\n","       [ 0, 61]])"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["print(classification_report(single_y_test, single_y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilHE5B8N16t-","executionInfo":{"status":"ok","timestamp":1679366353588,"user_tz":-540,"elapsed":1,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"1798e644-cde5-4aff-8ce6-89f11e6619e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      0.05      0.10        60\n","           1       0.52      1.00      0.68        61\n","\n","    accuracy                           0.53       121\n","   macro avg       0.76      0.53      0.39       121\n","weighted avg       0.76      0.53      0.39       121\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"qRoacK2mcLPb"},"source":["### (4) 모델2\n","- **세부요구사항**\n","    - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 validation_data로 validation set을 사용하시오.\n","    - 반드시 Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["#### 1) 구조 설계"],"metadata":{"id":"5WTwG8NFoLBQ"}},{"cell_type":"markdown","source":["![알뤡스](https://cdn-images-1.medium.com/max/1600/1*jqKHgwZ8alM3K_JRYO_l4w.png)"],"metadata":{"id":"cnbFEcB04KPB"}},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"DqTzgRTroLBR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcVDXnpQoLBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679368840496,"user_tz":-540,"elapsed":718,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"cdd972ff-a812-474e-c4fb-1cc0de4a3bbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 227, 227, 3)]     0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n"," )                                                               \n","                                                                 \n"," batch_normalization (BatchN  (None, 27, 27, 96)       384       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 27, 27, 512)       1229312   \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 13, 13, 512)      0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 13, 13, 512)      2048      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 13, 13, 768)       3539712   \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 13, 13, 768)       5309184   \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 13, 13, 512)       3539456   \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 6, 6, 512)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 6, 6, 512)        2048      \n"," hNormalization)                                                 \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dense (Dense)               (None, 256)               131328    \n","                                                                 \n"," dense_1 (Dense)             (None, 128)               32896     \n","                                                                 \n"," dense_2 (Dense)             (None, 128)               16512     \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 258       \n","                                                                 \n","=================================================================\n","Total params: 13,838,082\n","Trainable params: 13,835,842\n","Non-trainable params: 2,240\n","_________________________________________________________________\n"]}],"source":["# 1. 세션 클리어\n","keras.backend.clear_session()\n","\n","# 2. 모델 사슬처럼 엮기\n","il = Input(shape = (227, 227, 3))\n","hl = Conv2D( filters = 96, kernel_size = (11, 11), strides = (4, 4), activation = 'relu')(il)\n","hl = MaxPool2D( pool_size = (3, 3), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 512, kernel_size = (5, 5), padding = 'same', activation = 'relu')(hl)\n","hl = MaxPool2D( pool_size = (3, 3), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 768, kernel_size = (3, 3), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 768, kernel_size = (3, 3), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), padding = 'same', activation = 'relu')(hl)\n","hl = MaxPool2D( pool_size = (3, 3), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","# hl = keras.layers.GlobalAvgPool2D()(hl)\n","hl = Faltten()\n","\n","hl = Dense(9216, activation = 'relu')(hl)\n","hl = Dense(4096, activation = 'relu')(hl)\n","hl = Dense(4096, activation = 'relu')(hl)\n","\n","hl = Dropout(0.25)(hl)\n","\n","ol = Dense(2, activation = 'sigmoid')(hl)\n","\n","model_alexnet = keras.models.Model(il, ol)\n","\n","\n","#  4. 모델 컴파일\n","model_alexnet.compile(loss = 'binary_crossentropy', metrics = ['accuracy'],\n","              optimizer = 'adam')\n","# 모델 요약\n","model_alexnet.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAhXnGmXoLBS"},"outputs":[],"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","es = EarlyStopping(monitor = 'val_loss',\n","                   min_delta = 0,\n","                   verbose = 1,\n","                   patience = 7,\n","                   restore_best_weights = True\n","                   )"]},{"cell_type":"code","source":["mcp = ModelCheckpoint(filepath = '/content/drive/MyDrive/Datasets/mcp2.h5',\n","                      monitor = 'val_loss',\n","                      verbose = 1,\n","                      save_best_only = True,\n","                      save_weights_only = False\n","                      )"],"metadata":{"id":"EwH-NcdHIq2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_alexnet.fit(x_train, y_train, callbacks = [es, mcp], epochs = 1000, validation_data = (x_valid, y_valid), verbose = 1, batch_size = 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GE2PbYkj9FZQ","executionInfo":{"status":"ok","timestamp":1679368908996,"user_tz":-540,"elapsed":38010,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"267c2643-9145-4328-8cd0-bfcb8403bc12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.7809\n","Epoch 1: val_loss improved from inf to 16.81968, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 8s 220ms/step - loss: 0.4834 - accuracy: 0.7809 - val_loss: 16.8197 - val_accuracy: 0.5000\n","Epoch 2/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3954 - accuracy: 0.8516\n","Epoch 2: val_loss improved from 16.81968 to 5.11182, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 125ms/step - loss: 0.4032 - accuracy: 0.8454 - val_loss: 5.1118 - val_accuracy: 0.5000\n","Epoch 3/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3733 - accuracy: 0.8411\n","Epoch 3: val_loss improved from 5.11182 to 3.50659, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 126ms/step - loss: 0.3736 - accuracy: 0.8428 - val_loss: 3.5066 - val_accuracy: 0.5000\n","Epoch 4/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3685 - accuracy: 0.8542\n","Epoch 4: val_loss improved from 3.50659 to 0.84437, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 126ms/step - loss: 0.3698 - accuracy: 0.8505 - val_loss: 0.8444 - val_accuracy: 0.5208\n","Epoch 5/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3683 - accuracy: 0.8438\n","Epoch 5: val_loss improved from 0.84437 to 0.75002, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 127ms/step - loss: 0.3676 - accuracy: 0.8428 - val_loss: 0.7500 - val_accuracy: 0.5208\n","Epoch 6/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3585 - accuracy: 0.8464\n","Epoch 6: val_loss improved from 0.75002 to 0.60922, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 127ms/step - loss: 0.3574 - accuracy: 0.8454 - val_loss: 0.6092 - val_accuracy: 0.6562\n","Epoch 7/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3672 - accuracy: 0.8464\n","Epoch 7: val_loss improved from 0.60922 to 0.54012, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 134ms/step - loss: 0.3674 - accuracy: 0.8454 - val_loss: 0.5401 - val_accuracy: 0.6667\n","Epoch 8/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3826 - accuracy: 0.8464\n","Epoch 8: val_loss did not improve from 0.54012\n","13/13 [==============================] - 1s 88ms/step - loss: 0.3813 - accuracy: 0.8479 - val_loss: 1.3759 - val_accuracy: 0.5521\n","Epoch 9/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3222 - accuracy: 0.8646\n","Epoch 9: val_loss improved from 0.54012 to 0.52991, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 127ms/step - loss: 0.3367 - accuracy: 0.8634 - val_loss: 0.5299 - val_accuracy: 0.7917\n","Epoch 10/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3060 - accuracy: 0.8698\n","Epoch 10: val_loss improved from 0.52991 to 0.38547, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 128ms/step - loss: 0.3127 - accuracy: 0.8686 - val_loss: 0.3855 - val_accuracy: 0.8646\n","Epoch 11/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3174 - accuracy: 0.8724\n","Epoch 11: val_loss did not improve from 0.38547\n","13/13 [==============================] - 1s 88ms/step - loss: 0.3189 - accuracy: 0.8711 - val_loss: 1.1347 - val_accuracy: 0.5312\n","Epoch 12/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3426 - accuracy: 0.8516\n","Epoch 12: val_loss did not improve from 0.38547\n","13/13 [==============================] - 1s 89ms/step - loss: 0.3439 - accuracy: 0.8505 - val_loss: 0.5826 - val_accuracy: 0.6354\n","Epoch 13/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3024 - accuracy: 0.8568\n","Epoch 13: val_loss did not improve from 0.38547\n","13/13 [==============================] - 1s 88ms/step - loss: 0.3014 - accuracy: 0.8582 - val_loss: 0.5050 - val_accuracy: 0.7396\n","Epoch 14/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3167 - accuracy: 0.8672\n","Epoch 14: val_loss did not improve from 0.38547\n","13/13 [==============================] - 1s 88ms/step - loss: 0.3161 - accuracy: 0.8660 - val_loss: 0.3989 - val_accuracy: 0.8438\n","Epoch 15/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3020 - accuracy: 0.8776\n","Epoch 15: val_loss did not improve from 0.38547\n","13/13 [==============================] - 1s 87ms/step - loss: 0.3054 - accuracy: 0.8763 - val_loss: 0.6688 - val_accuracy: 0.6146\n","Epoch 16/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.3089 - accuracy: 0.8698\n","Epoch 16: val_loss improved from 0.38547 to 0.34457, saving model to /content/drive/MyDrive/Datasets/mcp2.h5\n","13/13 [==============================] - 2s 132ms/step - loss: 0.3113 - accuracy: 0.8686 - val_loss: 0.3446 - val_accuracy: 0.8542\n","Epoch 17/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.2758 - accuracy: 0.8880\n","Epoch 17: val_loss did not improve from 0.34457\n","13/13 [==============================] - 1s 88ms/step - loss: 0.2760 - accuracy: 0.8866 - val_loss: 0.6712 - val_accuracy: 0.7708\n","Epoch 18/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.2795 - accuracy: 0.8854\n","Epoch 18: val_loss did not improve from 0.34457\n","13/13 [==============================] - 1s 87ms/step - loss: 0.2791 - accuracy: 0.8866 - val_loss: 0.7697 - val_accuracy: 0.7708\n","Epoch 19/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.2571 - accuracy: 0.8776\n","Epoch 19: val_loss did not improve from 0.34457\n","13/13 [==============================] - 1s 87ms/step - loss: 0.2718 - accuracy: 0.8711 - val_loss: 0.4813 - val_accuracy: 0.7292\n","Epoch 20/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.2764 - accuracy: 0.8698\n","Epoch 20: val_loss did not improve from 0.34457\n","13/13 [==============================] - 1s 87ms/step - loss: 0.2815 - accuracy: 0.8660 - val_loss: 0.4399 - val_accuracy: 0.8229\n","Epoch 21/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.2690 - accuracy: 0.8776\n","Epoch 21: val_loss did not improve from 0.34457\n","13/13 [==============================] - 1s 87ms/step - loss: 0.2715 - accuracy: 0.8763 - val_loss: 0.4549 - val_accuracy: 0.7917\n","Epoch 22/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.2795 - accuracy: 0.9010\n","Epoch 22: val_loss did not improve from 0.34457\n","13/13 [==============================] - 1s 87ms/step - loss: 0.2902 - accuracy: 0.8995 - val_loss: 0.4537 - val_accuracy: 0.7396\n","Epoch 23/1000\n","12/13 [==========================>...] - ETA: 0s - loss: 0.2755 - accuracy: 0.9089Restoring model weights from the end of the best epoch: 16.\n","\n","Epoch 23: val_loss did not improve from 0.34457\n","13/13 [==============================] - 1s 91ms/step - loss: 0.2748 - accuracy: 0.9072 - val_loss: 0.5046 - val_accuracy: 0.7917\n","Epoch 23: early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7faa9878d6a0>"]},"metadata":{},"execution_count":79}]},{"cell_type":"markdown","source":["#### 3) test set으로 예측하고 평가하기\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"qxZ0U7K1oLBS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShruikbsoLBS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679368915236,"user_tz":-540,"elapsed":1594,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"6a5995f1-b3db-40f6-ab35-396c0dfdb40b"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 1s 259ms/step\n"]}],"source":["y_pred = model_alexnet.predict(x_test)\n","\n","single_y_pred = y_pred.argmax(axis=1)\n","single_y_test = y_test.argmax(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8MC8l07oLBS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679368917470,"user_tz":-540,"elapsed":458,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"55d72914-d41d-4fb2-fed4-5c516ebe9cf0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[49, 11],\n","       [ 8, 53]])"]},"metadata":{},"execution_count":81}],"source":["confusion_matrix(single_y_test, single_y_pred)"]},{"cell_type":"code","source":["print(classification_report(single_y_test, single_y_pred))"],"metadata":{"id":"G7FVsFiCFSGa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679368919331,"user_tz":-540,"elapsed":2,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"53ef3b12-6299-403a-d566-0709330a08d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.86      0.82      0.84        60\n","           1       0.83      0.87      0.85        61\n","\n","    accuracy                           0.84       121\n","   macro avg       0.84      0.84      0.84       121\n","weighted avg       0.84      0.84      0.84       121\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lV1MZbVAIox-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRqzBw8eccwj"},"source":["### (5) 모델3 - VGG-16(D구조)"]},{"cell_type":"markdown","source":["![VGG-16(D구조)**굵은 텍스트**](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FK990l%2FbtqwDJ7C54R%2F664Ksm6gyTGBR1wK3YPDFk%2Fimg.png)"],"metadata":{"id":"LtNd8u5RoNJo"}},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"4zgVkXLHoNJo"}},{"cell_type":"code","execution_count":38,"metadata":{"id":"gTlUNbkhoNJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376305002,"user_tz":-540,"elapsed":4082,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"4a4b4970-85fe-45f7-d0e6-77663cf141bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n"," )                                                               \n","                                                                 \n"," batch_normalization (BatchN  (None, 112, 112, 64)     256       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 56, 56, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080    \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 28, 28, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808   \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 14, 14, 512)      2048      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808   \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 7, 7, 512)        2048      \n"," hNormalization)                                                 \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 4096)              102764544 \n","                                                                 \n"," dense_1 (Dense)             (None, 4096)              16781312  \n","                                                                 \n"," dense_2 (Dense)             (None, 1000)              4097000   \n","                                                                 \n"," dropout (Dropout)           (None, 1000)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 2002      \n","                                                                 \n","=================================================================\n","Total params: 138,365,434\n","Trainable params: 138,362,490\n","Non-trainable params: 2,944\n","_________________________________________________________________\n"]}],"source":["# 1. 세션 클리어\n","keras.backend.clear_session()\n","\n","# 2. 모델 사슬처럼 엮기\n","il = Input(shape = (224, 224, 3))\n","hl = Conv2D( filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(il)\n","hl = Conv2D( filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 128, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 128, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","# hl = keras.layers.GlobalAvgPool2D()(hl)\n","hl = Flatten()(hl)\n","\n","hl = Dense(4096, activation = 'relu')(hl)\n","hl = Dense(4096, activation = 'relu')(hl)\n","hl = Dense(1000, activation = 'relu')(hl)\n","\n","hl = Dropout(0.25)(hl)\n","\n","ol = Dense(2, activation = 'sigmoid')(hl)\n","\n","model_vgg_16 = keras.models.Model(il, ol)\n","\n","\n","#  4. 모델 컴파일\n","model_vgg_16.compile(loss = 'binary_crossentropy', metrics = ['accuracy'],\n","              optimizer = 'adam')\n","# 모델 요약\n","model_vgg_16.summary()"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"T4GYo0dboNJo","executionInfo":{"status":"ok","timestamp":1679376316765,"user_tz":-540,"elapsed":924,"user":{"displayName":"현태","userId":"11869159444039358312"}}},"outputs":[],"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","es = EarlyStopping(monitor = 'val_loss',\n","                   min_delta = 0,\n","                   verbose = 1,\n","                   patience = 15,\n","                   restore_best_weights = True\n","                   )\n","\n","mcp = ModelCheckpoint(filepath = '/content/drive/MyDrive/Datasets/mcp_vgg_16.h5',\n","                      monitor = 'val_loss',\n","                      verbose = 1,\n","                      save_best_only = True,\n","                      save_weights_only = False\n","                      )"]},{"cell_type":"code","source":["model_vgg_16.fit(x_train, y_train, callbacks = [es, mcp], epochs = 1000, validation_data = (x_valid, y_valid), verbose = 1, batch_size = 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1vjmSiTdmmG","outputId":"eb0e92b8-9d3c-4499-b0b6-c5fce647fc6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","13/13 [==============================] - ETA: 0s - loss: 4.9880 - accuracy: 0.6082\n","Epoch 1: val_loss improved from inf to 159.24754, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n","13/13 [==============================] - 51s 2s/step - loss: 4.9880 - accuracy: 0.6082 - val_loss: 159.2475 - val_accuracy: 0.5000\n","Epoch 2/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.6959\n","Epoch 2: val_loss improved from 159.24754 to 102.99628, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n","13/13 [==============================] - 13s 1s/step - loss: 0.6430 - accuracy: 0.6959 - val_loss: 102.9963 - val_accuracy: 0.5000\n","Epoch 3/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.7912\n","Epoch 3: val_loss improved from 102.99628 to 22.43327, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n","13/13 [==============================] - 13s 1s/step - loss: 0.4806 - accuracy: 0.7912 - val_loss: 22.4333 - val_accuracy: 0.5000\n","Epoch 4/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.8119\n","Epoch 4: val_loss improved from 22.43327 to 8.82624, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n","13/13 [==============================] - 13s 1s/step - loss: 0.4178 - accuracy: 0.8119 - val_loss: 8.8262 - val_accuracy: 0.5000\n","Epoch 5/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.7964\n","Epoch 5: val_loss did not improve from 8.82624\n","13/13 [==============================] - 6s 437ms/step - loss: 0.4391 - accuracy: 0.7964 - val_loss: 11.5966 - val_accuracy: 0.5000\n","Epoch 6/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3797 - accuracy: 0.8119\n","Epoch 6: val_loss improved from 8.82624 to 0.90137, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n","13/13 [==============================] - 13s 1s/step - loss: 0.3797 - accuracy: 0.8119 - val_loss: 0.9014 - val_accuracy: 0.5000\n","Epoch 7/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.8170\n","Epoch 7: val_loss improved from 0.90137 to 0.85824, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n","13/13 [==============================] - 13s 1s/step - loss: 0.3948 - accuracy: 0.8170 - val_loss: 0.8582 - val_accuracy: 0.5000\n","Epoch 8/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8479\n","Epoch 8: val_loss improved from 0.85824 to 0.74085, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n","13/13 [==============================] - 13s 1s/step - loss: 0.3536 - accuracy: 0.8479 - val_loss: 0.7409 - val_accuracy: 0.4792\n","Epoch 9/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.7603\n","Epoch 9: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 455ms/step - loss: 0.5343 - accuracy: 0.7603 - val_loss: 4.6193 - val_accuracy: 0.5000\n","Epoch 10/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.8557\n","Epoch 10: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 463ms/step - loss: 0.3498 - accuracy: 0.8557 - val_loss: 1.9581 - val_accuracy: 0.5000\n","Epoch 11/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8454\n","Epoch 11: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 472ms/step - loss: 0.3421 - accuracy: 0.8454 - val_loss: 1.8871 - val_accuracy: 0.2917\n","Epoch 12/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8660\n","Epoch 12: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 472ms/step - loss: 0.3379 - accuracy: 0.8660 - val_loss: 2.8013 - val_accuracy: 0.5000\n","Epoch 13/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.8995\n","Epoch 13: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 463ms/step - loss: 0.2414 - accuracy: 0.8995 - val_loss: 4.1779 - val_accuracy: 0.5000\n","Epoch 14/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.8918\n","Epoch 14: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 455ms/step - loss: 0.2608 - accuracy: 0.8918 - val_loss: 3.9739 - val_accuracy: 0.5000\n","Epoch 15/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.8995\n","Epoch 15: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 452ms/step - loss: 0.2362 - accuracy: 0.8995 - val_loss: 2.1177 - val_accuracy: 0.4688\n","Epoch 16/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.8969\n","Epoch 16: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 451ms/step - loss: 0.2399 - accuracy: 0.8969 - val_loss: 2.5712 - val_accuracy: 0.5521\n","Epoch 17/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3581 - accuracy: 0.8892\n","Epoch 17: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 449ms/step - loss: 0.3581 - accuracy: 0.8892 - val_loss: 4.3294 - val_accuracy: 0.5104\n","Epoch 18/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.8686\n","Epoch 18: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 449ms/step - loss: 0.3854 - accuracy: 0.8686 - val_loss: 2.2754 - val_accuracy: 0.5208\n","Epoch 19/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8273\n","Epoch 19: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 448ms/step - loss: 0.4162 - accuracy: 0.8273 - val_loss: 1.0416 - val_accuracy: 0.5000\n","Epoch 20/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8505\n","Epoch 20: val_loss did not improve from 0.74085\n","13/13 [==============================] - 6s 449ms/step - loss: 0.3646 - accuracy: 0.8505 - val_loss: 5.9707 - val_accuracy: 0.5000\n","Epoch 21/1000\n","13/13 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9149\n","Epoch 21: val_loss improved from 0.74085 to 0.73780, saving model to /content/drive/MyDrive/Datasets/mcp_vgg_16.h5\n"]}]},{"cell_type":"markdown","source":["#### 3) test set으로 예측하고 평가하기\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"uZV9zbsroNJo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc9UmjZ0oNJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679375386594,"user_tz":-540,"elapsed":1725,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"b7e5ab88-7741-4a1d-b942-bb5cd51c2fdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 1s 181ms/step\n"]}],"source":["y_pred = model_vgg_16.predict(x_test)\n","\n","single_y_pred = y_pred.argmax(axis=1)\n","single_y_test = y_test.argmax(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aP-p0_y9oNJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679374291499,"user_tz":-540,"elapsed":372,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"722bab63-2add-483f-8d9d-97dcca4fc1d9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[34, 26],\n","       [ 8, 53]])"]},"metadata":{},"execution_count":115}],"source":["confusion_matrix(single_y_test, single_y_pred)"]},{"cell_type":"code","source":["print(classification_report(single_y_test, single_y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Si6XbIxfNXn","executionInfo":{"status":"ok","timestamp":1679374294302,"user_tz":-540,"elapsed":7,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"5cd8c0d7-34f8-4be6-b84a-440b3667d6c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.81      0.57      0.67        60\n","           1       0.67      0.87      0.76        61\n","\n","    accuracy                           0.72       121\n","   macro avg       0.74      0.72      0.71       121\n","weighted avg       0.74      0.72      0.71       121\n","\n"]}]},{"cell_type":"markdown","source":["### (6) 모델4 - VGG-19(E구조)\n","![VGG-19](https://wikidocs.net/images/page/164796/vgg_Fig_07.jpeg)"],"metadata":{"id":"Yx4sNJxwiRWk"}},{"cell_type":"code","source":[],"metadata":{"id":"IyN5oIEmitZ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"g5RfrkXuiths"}},{"cell_type":"code","source":["# 1. 세션 클리어\n","keras.backend.clear_session()\n","\n","# 2. 모델 사슬처럼 엮기\n","il = Input(shape = (224, 224, 3))\n","hl = Conv2D( filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(il)\n","hl = Conv2D( filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 128, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 128, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","hl = Conv2D( filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu')(hl)\n","\n","hl = MaxPool2D( pool_size = (2, 2), strides = (2, 2))(hl)\n","hl = BatchNormalization()(hl)\n","\n","hl = Flatten()(hl)\n","\n","hl = Dense(4096, activation = 'relu')(hl)\n","hl = Dense(4096, activation = 'relu')(hl)\n","hl = Dense(1000, activation = 'relu')(hl)\n","\n","hl = Dropout(0.25)(hl)\n","\n","ol = Dense(2, activation = 'sigmoid')(hl)\n","\n","model_vgg_19 = keras.models.Model(il, ol)\n","\n","\n","#  4. 모델 컴파일\n","model_vgg_19.compile(loss = 'binary_crossentropy', metrics = ['accuracy'],\n","              optimizer = 'adam')\n","# 모델 요약\n","model_vgg_19.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"3uGeOK1GiucG","executionInfo":{"status":"error","timestamp":1679375781551,"user_tz":-540,"elapsed":10934,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"e2ad6237-7eda-4416-80f9-8412f642e589"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-131-46e95f677cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mhl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mhl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mhl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mhl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m   2101\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]"]}]},{"cell_type":"code","source":["from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","es = EarlyStopping(monitor = 'val_loss',\n","                   min_delta = 0,\n","                   verbose = 1,\n","                   patience = 15,\n","                   restore_best_weights = True\n","                   )\n","\n","mcp = ModelCheckpoint(filepath = '/content/drive/MyDrive/Datasets/mcp_vgg_19.h5',\n","                      monitor = 'val_loss',\n","                      verbose = 1,\n","                      save_best_only = True,\n","                      save_weights_only = False\n","                      )"],"metadata":{"id":"xmkzCRFWkG9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_vgg_19.fit(x_train, y_train, callbacks = [es, mcp], epochs = 1000, validation_data = (x_valid, y_valid), verbose = 1, batch_size = 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"T81okNkckNN6","executionInfo":{"status":"error","timestamp":1679375289236,"user_tz":-540,"elapsed":18293,"user":{"displayName":"현태","userId":"11869159444039358312"}},"outputId":"85286ffa-4a67-4223-b524-af0820ecfa89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n"]},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-124-4684e445bcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_vgg_19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-124-4684e445bcb9>\", line 1, in <module>\n      model_vgg_19.fit(x_train, y_train, callbacks = [es, mcp], epochs = 1000, validation_data = (x_valid, y_valid), verbose = 1, batch_size = 32)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 526, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 259, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput'\nOOM when allocating tensor with shape[32,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_115129]"]}]},{"cell_type":"markdown","metadata":{"id":"AxUpfhJ1xXle"},"source":["## 4.모델링 II\n","* **세부요구사항**\n","    - 성능을 높이기 위해서 다음의 두가지를 시도해 봅시다.\n","        - Data Augmentation을 통해 데이터를 증가 시킵니다.\n","            - ImageDataGenerator를 사용합니다.\n","        - 사전 학습된 모델(Transfer Learning)을 가져다 사용해 봅시다.\n","            - VGG16(이미지넷)을 사용해 봅시다."]},{"cell_type":"markdown","metadata":{"id":"ouCRBdKPxCut"},"source":["### (1) Data Augmentation\n","- **세부요구사항**\n","    * 모델 학습에 이용할 이미지 데이터를 증강시키세요.\n","    * Keras의 ImageDataGenerator를 이용\n","        - [ImageDataGenerator document](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n","\n","    * image generator를 이용하여 학습\n","        * 모델 구조는 이미 생성한 1,2,3 중 하나를 선택하여 학습\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qe6yjs8F7Zox"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYae9YFt8Q03"},"outputs":[],"source":["img_size = 280 ## 사이즈 조정 가능\n","\n","train_path = dataset_path+'Car_Images_train/'\n","valid_path = dataset_path+'Car_Images_valid/'"]},{"cell_type":"markdown","source":["#### 1) ImageGenerator 생성\n","* ImageDataGenerator 함수 사용\n","    * 주요 옵션\n","        * rotation_range: 무작위 회전을 적용할 각도 범위\n","        * zoom_range: 무작위 줌을 적용할 범위 [1-zoom_range, 1+zoom_range]\n","        * horizontal_flip: 무작위 좌우반전을 적용할지 여부\n","        * vertical_flip: 무작위 상하반전을 적용할지 여부\n","        * rescale: 텐서의 모든 값을 rescale 값으로 나누어줌 (이 경우에는 255로 나누어서 0~1사이의 값으로 변경)"],"metadata":{"id":"IP4jIyTGfXD_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKPPSwYn7Zrj"},"outputs":[],"source":["train_datagen = \n","\n","valid_datagen = \n"]},{"cell_type":"markdown","source":["#### 2) 경로로 부터 이미지 불러 올 준비\n","* .flow_from_directory 이용\n","    * 디렉토리에서 이미지를 가져와서 데이터 증강을 적용하고 batch 단위로 제공하는 generator를 생성합니다.\n","    * 이미지를 불러올 때 target_size로 크기를 맞추고, \n","    * class_mode로 이진 분류(binary)를 수행하도록 지정합니다.\n"],"metadata":{"id":"dKwSYYkufanb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bwvQ4hHSCwY"},"outputs":[],"source":["train_generator = \n","\n","valid_generator = \n"]},{"cell_type":"markdown","metadata":{"id":"g4RPCjU5f662"},"source":["#### 3) 학습\n","- **세부요구사항**\n","    - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 train_generator 이용. \n","    - validation_data = valid_generator 지정\n","    - Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["* 구조 설계"],"metadata":{"id":"wVMLsXw6f663"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_W7rqgH1f663"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["* 학습\n","    * EarlyStopping 설정하기\n","    * 학습 데이터에 train_generator, validation_data=valid_generator 사용"],"metadata":{"id":"nw2_G7zdf663"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6m5mRE9Nf663"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCWzBSYqf663"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#### 4) 성능 평가\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"BdKiY1uIf663"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qjnvt0lf663"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBl4Do0af663"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"S1iv22vSxXle"},"source":["### (2) Transfer Learning\n","- **세부요구사항**\n","    * VGG16 모델은 1000개의 클래스를 분류하는 데 사용된 ImageNet 데이터셋을 기반으로 사전 학습된 가중치를 가지고 있습니다. \n","        * 따라서 이 모델은 이미지 분류 문제에 대한 높은 성능을 보입니다.\n","        * 이 모델은 보통 전이학습(transfer learning)에서 기본적으로 사용되며, 특히 대규모 데이터셋이 없을 때는 기본 모델로 사용되어 fine-tuning을 수행합니다.\n","    * VGG16 함수로 부터 base_model 저장\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnS12YhDxXle"},"outputs":[],"source":["from tensorflow.keras.applications import VGG16"]},{"cell_type":"markdown","source":["#### 1) VGG16 불러와서 저장하기\n","* include_top=False로 설정하여 분류기를 제외하고 미리 학습된 가중치 imagenet을 로드합니다.\n","* .trainable을 True로 설정하여 모델의 모든 레이어들이 fine-tuning에 대해 업데이트되도록 합니다.\n"],"metadata":{"id":"d3kyvCwIiAfi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFf3IxbBGe9B"},"outputs":[],"source":["base_model = VGG16(                 )\n","\n","\n"]},{"cell_type":"markdown","source":["#### 2) VGG16과 연결한 구조 설계\n","* VGG16을 불러와서 Flatten, Dense 등으로 레이어 연결하기"],"metadata":{"id":"D-JjBLZZiIxA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yg4KhHQ8xXlf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5V5heiDxxXlf"},"source":["#### 3) 학습\n","- **세부요구사항**\n","    - 모델 학습 과정에 알맞은 보조 지표를 사용하세요.\n","    - 데이터\n","        * Image Generator를 연결하거나\n","        * 기존 train, validation 셋을 이용해도 됩니다.\n","        - Early Stopping을 반드시 사용하세요.\n","        - 최적의 가중치를 모델에 적용하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtqQIS-HxXlg"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zg0L88Gwf4l"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#### 4) 성능 평가"],"metadata":{"id":"BbhiWcS5i735"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik4AFzCQi735"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkkSsyMoi735"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGuQMUJNxXSy"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}